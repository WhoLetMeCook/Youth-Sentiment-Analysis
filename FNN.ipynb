{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/seastoexist/feedfoward-nn?scriptVersionId=191472420\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom datasets import load_dataset, Dataset\nfrom nltk.tokenize import word_tokenize\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport gc\n\nnltk.download('punkt')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T02:45:12.931252Z","iopub.execute_input":"2024-08-07T02:45:12.932112Z","iopub.status.idle":"2024-08-07T02:45:19.721465Z","shell.execute_reply.started":"2024-08-07T02:45:12.932071Z","shell.execute_reply":"2024-08-07T02:45:19.720171Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Load datasets\nimdb = load_dataset('stanfordnlp/imdb')\nsst2 = load_dataset('glue', 'sst2')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T02:45:19.724304Z","iopub.execute_input":"2024-08-07T02:45:19.724929Z","iopub.status.idle":"2024-08-07T02:45:26.997405Z","shell.execute_reply.started":"2024-08-07T02:45:19.724893Z","shell.execute_reply":"2024-08-07T02:45:26.996229Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0b7e52de20f4b06b89745de10238f3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d695fd9ad4d14cbeb73b620422b8b717"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"660a83b24c8246c59edd4d87dfe273ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef7bcc0153854b82b965703617f1ee7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5575092b663448d6b2396e4a55ed4617"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47349778a50b42d5a075d5f5ab40f6cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dea7af22bdd4bb887bb7abe01c00a89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c8f0cfb728444d39a2bc89c8f6d1543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c9dd85fe55b44a59865aad336395746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a327f6719d34d1b8389fe8652f8cf92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"654ce6b0a3ef470bb1db55947408413d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e78c0c59dfef40168ab9aec38c18511c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7df9141702374fd0883888c0de9691df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a71bb4004e034d2e995acee168d73f9c"}},"metadata":{}}]},{"cell_type":"code","source":"# Prepare dataset splits\nimdbTr = imdb['train']\nimdbTe = imdb['test']\nsst2Tr = sst2['train']\nsst2Te = sst2['validation']\n\n# Convert datasets to lists\nimdbTrL = [example for example in imdbTr]\nimdbTeL = [example for example in imdbTe]\nsst2TrL = [example for example in sst2Tr]\nsst2TeL = [example for example in sst2Te]\n\n#Append datasets\ntrainList = imdbTrL + sst2TrL\ntestList = imdbTeL + sst2TeL\n\ntrain = Dataset.from_list(trainList)\ntest = Dataset.from_list(testList)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T02:45:26.998897Z","iopub.execute_input":"2024-08-07T02:45:26.999286Z","iopub.status.idle":"2024-08-07T02:45:27.004988Z","shell.execute_reply.started":"2024-08-07T02:45:26.999254Z","shell.execute_reply":"2024-08-07T02:45:27.003745Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Add is_long feature to differentiate between sst2 and imdb samples\nsth = 100\ndef add_is_long(example, threshold=sth):\n    text = example.get('text', example.get('sentence', ''))\n    tokens = word_tokenize(str(text))\n    example['is_long'] = int(len(tokens) > threshold)\n    return example\n\ntrain = train.map(add_is_long)\ntest = test.map(add_is_long)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T02:45:32.384462Z","iopub.execute_input":"2024-08-07T02:45:32.384832Z","iopub.status.idle":"2024-08-07T02:47:27.863345Z","shell.execute_reply.started":"2024-08-07T02:45:32.384799Z","shell.execute_reply":"2024-08-07T02:47:27.862238Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/92349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ed7e69226649bda9a2827d84ce9767"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea91f49e892a4836be3f96f2616a586c"}},"metadata":{}}]},{"cell_type":"code","source":"# Define neural network\nclass FeedforwardNN(nn.Module):\n    def __init__(self, input_dim):\n        super(FeedforwardNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 256)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.fc2 = nn.Linear(256, 128)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.fc3 = nn.Linear(128, 1) #2 hidden layers\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.5)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        out = self.fc2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        out = self.fc3(out)\n        out = self.sigmoid(out)\n        print(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-08-07T02:47:27.86467Z","iopub.execute_input":"2024-08-07T02:47:27.864979Z","iopub.status.idle":"2024-08-07T02:47:27.873671Z","shell.execute_reply.started":"2024-08-07T02:47:27.864954Z","shell.execute_reply":"2024-08-07T02:47:27.872365Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def run_kfold(k, trainList):\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n    fold_accuracies = []\n    fold_losses = []\n\n    for fold, (train_index, val_index) in enumerate(kf.split(trainList)):\n        train_data = [trainList[i] for i in train_index]\n        val_data = [trainList[i] for i in val_index]\n        \n        # Tfidf encoding\n        vectorizer = TfidfVectorizer(max_features=10000)\n        train_texts = [' '.join(word_tokenize(str(example.get('text', example.get('sentence', ''))))) for example in train_data]\n        val_texts = [' '.join(word_tokenize(str(example.get('text', example.get('sentence', ''))))) for example in val_data]\n        X_train_fold = vectorizer.fit_transform(train_texts).toarray()\n        X_val_fold = vectorizer.transform(val_texts).toarray()\n\n        #Add is_long feature\n        train_is_long = np.array([example.get('is_long', 0) for example in train_data]).reshape(-1, 1)\n        val_is_long = np.array([example.get('is_long', 0) for example in val_data]).reshape(-1, 1)\n        X_train_fold = np.hstack((X_train_fold, train_is_long))\n        X_val_fold = np.hstack((X_val_fold, val_is_long))\n\n        #scikit-learn StandardScaler to center data\n        scaler = StandardScaler()\n        X_train_fold = scaler.fit_transform(X_train_fold)\n        X_val_fold = scaler.transform(X_val_fold)\n\n        #Create tensors\n        X_train_tensor = torch.tensor(X_train_fold, dtype=torch.float32)\n        y_train_tensor = torch.tensor([example['label'] for example in train_data], dtype=torch.float32).unsqueeze(1)\n        X_val_tensor = torch.tensor(X_val_fold, dtype=torch.float32)\n        y_val_tensor = torch.tensor([example['label'] for example in val_data], dtype=torch.float32).unsqueeze(1)\n        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n\n        input_dim = X_train_fold.shape[1]\n        model = FeedforwardNN(input_dim)\n        \n        # BCE loss function\n        criterion = nn.BCELoss()\n        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\n        num_epochs = 50\n\n        #Accuracy calculation and epochs\n        for epoch in range(num_epochs):\n            model.train()\n            for inputs, labels in train_loader:\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            model.eval()\n            with torch.no_grad():\n                val_outputs = model(X_val_tensor)\n                val_loss = criterion(val_outputs, y_val_tensor).item()\n                val_outputs = (val_outputs > 0.5).float()\n                accuracy = (val_outputs.eq(y_val_tensor).sum().item() / len(y_val_tensor)) * 100\n\n            scheduler.step()\n\n        fold_accuracies.append(accuracy)\n        fold_losses.append(val_loss)\n\n        # Clear up space\n        del model, optimizer, X_train_fold, X_val_fold, y_train_tensor, y_val_tensor\n        del train_dataset, train_loader, X_train_tensor, X_val_tensor\n        gc.collect()\n\n    avg_accuracy = np.mean(fold_accuracies)\n    avg_loss = np.mean(fold_losses)\n    print(f'Average Accuracy: {avg_accuracy:.2f}%, Average Loss: {avg_loss:.4f}')\n\nk = 10\nrun_kfold(k, trainList)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-07T02:47:27.875125Z","iopub.execute_input":"2024-08-07T02:47:27.875523Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
